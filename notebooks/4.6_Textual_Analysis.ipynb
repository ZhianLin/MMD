{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mestrado em Modelagem Matematica da Informacao\n",
    "----------------------------------------------\n",
    "Disciplina: Modelagem e Mineracao de Dados\n",
    "------------------------------------------\n",
    "\n",
    "Master Program - Mathematical Modeling of Information\n",
    "-----------------------------------------------------\n",
    "Course: Data Mining and Modeling\n",
    "--------------------------------\n",
    "\n",
    "Professor: Renato Rocha Souza\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic: Textual Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the Python Packages used:  \n",
    "http://docs.python.org/library/re.html  \n",
    "http://www.pythonware.com/library/pil/handbook/index.htm  \n",
    "http://nltk.org/  \n",
    "https://networkx.github.io/  \n",
    "https://github.com/grangier/python-goose  \n",
    "https://pypi.python.org/pypi/Topics  \n",
    "http://radimrehurek.com/gensim/  \n",
    "http://docs.python-requests.org/en/latest/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import urllib\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import gensim\n",
    "\n",
    "from IPython.core.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "#from goose import Goose #https://github.com/codelucas/newspaper\n",
    "#from Topics.onlineldavb import onlineldavb\n",
    "#from Topics.visualization.wordcloud import make_wordcloud\n",
    "#from Topics.visualization.topiccloud import GenCloud\n",
    "#from Topics.visualization.printtopics import list_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying the path to the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = pathlib.Path(\"../datasets/\")\n",
    "outputs = pathlib.Path(\"../outputs/\")\n",
    "\n",
    "oplexicon = 'oplexicon_v3.0/lexico_v3.0.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Goose](https://github.com/goose3/goose3)\n",
    "\n",
    "Extracting text from html pages  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from goose3 import Goose\n",
    "\n",
    "url = 'http://edition.cnn.com/2012/02/22/world/europe/uk-occupy-london/index.html?hpt=ieu_c2'\n",
    "g = Goose(Goose({'use_meta_language': False, 'target_language':'en', 'parser_class':'lxml'}))\n",
    "\n",
    "article = g.extract(url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Occupy London loses eviction fight'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Occupy London protesters who have been camped outside the landmark St. Paul's Cathedral for the past four months lost their court bid to avoid eviction Wednesday in a decision made by London's Court of Appeal.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.meta_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Occupy London protesters who have been camped outside the landmark St. Paul's Cathedral for the past four months lost their court bid to avoid evictio\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.cleaned_text[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://cdn.cnn.com/cnnnext/dam/assets/111230083702-mclaughlin-uk-occupy-london-00021228-story-top.jpg'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.top_image.src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Textblob](http://textblob.readthedocs.io/en/dev/)\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "\n",
    "Installing:  \n",
    "\n",
    "!sudo pip3 install -U textblob  \n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "text_pt = '''\n",
    "Com uma abordagem inédita, o curso de Mestrado em Modelagem Matemática \n",
    "integra à Matemática Aplicada o corpo de conhecimentos das Ciências da \n",
    "Computação e da Informação, com contextos de aplicações das ciências \n",
    "sociais, econômicas, biológicas e da saúde. O curso possibilita ao \n",
    "mestrando desenvolver a capacidade de analisar cenários e dar suporte \n",
    "à tomada de decisões em situações de uso intensivo de dados e informações, \n",
    "além de ter o objetivo de formar excelentes pesquisadores na área.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(text)\n",
    "blob_pt = TextBlob(text_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " The titular threat of The Blob has always struck me as the ultimate movie\n",
       " monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
       " virtually any safeguard, capable of--as a doomed doctor chillingly\n",
       " describes it--\"assimilating flesh on contact.\"),\n",
       " Sentence(\"Snide comparisons to gelatin be damned, it's a concept with the most\n",
       " devastating of potential consequences, not unlike the grey goo scenario\n",
       " proposed by technological theorists fearful of\n",
       " artificial intelligence run rampant.\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " Com uma abordagem inédita, o curso de Mestrado em Modelagem Matemática \n",
       " integra à Matemática Aplicada o corpo de conhecimentos das Ciências da \n",
       " Computação e da Informação, com contextos de aplicações das ciências \n",
       " sociais, econômicas, biológicas e da saúde.\"),\n",
       " Sentence(\"O curso possibilita ao \n",
       " mestrando desenvolver a capacidade de analisar cenários e dar suporte \n",
       " à tomada de decisões em situações de uso intensivo de dados e informações, \n",
       " além de ter o objetivo de formar excelentes pesquisadores na área.\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_pt.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Snide', 'comparisons', 'to', 'gelatin', 'be', 'damned', 'it', \"'s\", 'a', 'concept', 'with', 'the', 'most', 'devastating', 'of', 'potential', 'consequences', 'not', 'unlike', 'the', 'grey', 'goo', 'scenario', 'proposed', 'by', 'technological', 'theorists', 'fearful', 'of', 'artificial', 'intelligence', 'run', 'rampant'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences[1].words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions applied to the Word object:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Snide', 'comparison', 'to', 'gelatin', 'be', 'damned', 'it', \"'\", 'a', 'concept', 'with', 'the', 'most', 'devastating', 'of', 'potential', 'consequence', 'not', 'unlike', 'the', 'grey', 'goo', 'scenario', 'proposed', 'by', 'technological', 'theorist', 'fearful', 'of', 'artificial', 'intelligence', 'run', 'rampant'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentences[1].words.singularize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platforms'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('Platform')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('running')\n",
    "w.lemmatize(\"v\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('titular', 'JJ'),\n",
       " ('threat', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('Blob', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('always', 'RB'),\n",
       " ('struck', 'VBN'),\n",
       " ('me', 'PRP')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Com', 'NNP'),\n",
       " ('uma', 'JJ'),\n",
       " ('abordagem', 'NN'),\n",
       " ('inédita', 'NN'),\n",
       " ('o', 'JJ'),\n",
       " ('curso', 'NN'),\n",
       " ('de', 'IN'),\n",
       " ('Mestrado', 'NNP'),\n",
       " ('em', 'CC'),\n",
       " ('Modelagem', 'NNP')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_pt.tags[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'titular']\n",
      "['titular', 'threat']\n",
      "['threat', 'of']\n",
      "['of', 'The']\n",
      "['The', 'Blob']\n",
      "['Blob', 'has']\n",
      "['has', 'always']\n",
      "['always', 'struck']\n",
      "['struck', 'me']\n",
      "['me', 'as']\n",
      "['as', 'the']\n",
      "['the', 'ultimate']\n",
      "['ultimate', 'movie']\n",
      "['movie', 'monster']\n",
      "['monster', 'an']\n",
      "['an', 'insatiably']\n",
      "['insatiably', 'hungry']\n",
      "['hungry', 'amoeba-like']\n",
      "['amoeba-like', 'mass']\n",
      "['mass', 'able']\n",
      "['able', 'to']\n",
      "['to', 'penetrate']\n",
      "['penetrate', 'virtually']\n",
      "['virtually', 'any']\n",
      "['any', 'safeguard']\n",
      "['safeguard', 'capable']\n",
      "['capable', 'of']\n",
      "['of', 'as']\n",
      "['as', 'a']\n",
      "['a', 'doomed']\n",
      "['doomed', 'doctor']\n",
      "['doctor', 'chillingly']\n",
      "['chillingly', 'describes']\n",
      "['describes', 'it']\n",
      "['it', 'assimilating']\n",
      "['assimilating', 'flesh']\n",
      "['flesh', 'on']\n",
      "['on', 'contact']\n",
      "['contact', 'Snide']\n",
      "['Snide', 'comparisons']\n",
      "['comparisons', 'to']\n",
      "['to', 'gelatin']\n",
      "['gelatin', 'be']\n",
      "['be', 'damned']\n",
      "['damned', 'it']\n",
      "['it', \"'s\"]\n",
      "[\"'s\", 'a']\n",
      "['a', 'concept']\n",
      "['concept', 'with']\n",
      "['with', 'the']\n",
      "['the', 'most']\n",
      "['most', 'devastating']\n",
      "['devastating', 'of']\n",
      "['of', 'potential']\n",
      "['potential', 'consequences']\n",
      "['consequences', 'not']\n",
      "['not', 'unlike']\n",
      "['unlike', 'the']\n",
      "['the', 'grey']\n",
      "['grey', 'goo']\n",
      "['goo', 'scenario']\n",
      "['scenario', 'proposed']\n",
      "['proposed', 'by']\n",
      "['by', 'technological']\n",
      "['technological', 'theorists']\n",
      "['theorists', 'fearful']\n",
      "['fearful', 'of']\n",
      "['of', 'artificial']\n",
      "['artificial', 'intelligence']\n",
      "['intelligence', 'run']\n",
      "['run', 'rampant']\n"
     ]
    }
   ],
   "source": [
    "for ngram in blob.ngrams(2):\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['titular threat', 'blob', 'ultimate movie monster', 'amoeba-like mass', 'snide', 'potential consequences', 'grey goo scenario', 'technological theorists fearful', 'artificial intelligence run rampant'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['com', 'uma abordagem inédita', 'o curso', 'mestrado', 'modelagem matemática', 'integra à', 'matemática aplicada', 'o corpo', 'conhecimentos das', 'ciências', 'computação', 'e da', 'informação', 'com contextos', 'aplicações das ciências sociais', 'biológicas e da saúde', 'o curso possibilita ao mestrando desenvolver', 'analisar cenários e dar suporte à tomada', 'decisões em situações', 'uso intensivo', 'dados e informações', 'ter o objetivo', 'formar excelentes pesquisadores na área'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_pt.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06000000000000001\n",
      "-0.34166666666666673\n"
     ]
    }
   ],
   "source": [
    "for sentence in blob.sentences:\n",
    "    print(sentence.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"A ameaça titular de The Blob sempre me pareceu o melhor filme\n",
       "Monstro: uma massa insaciável com fome e ameba capaz de penetrar\n",
       "praticamente qualquer salvaguarda, capaz de - como um doutor doente com calma\n",
       "descreve isso - \"assimilando a carne no contato\".\n",
       "As combinações de Snide com a gelatina serão condenadas, é um conceito com o máximo\n",
       "devastador de possíveis consequências, não muito diferente do cenário cinza\n",
       "proposto por teóricos tecnológicos com medo de\n",
       "a inteligência artificial corre desenfreada.\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.translate(to=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pt'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_pt.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spellcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"FGV is the best think ten in the word!\")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob('FGV is the bist think tenk in the word!')\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ten', 0.6656534954407295),\n",
       " ('tend', 0.1580547112462006),\n",
       " ('tent', 0.0729483282674772),\n",
       " ('tens', 0.0486322188449848),\n",
       " ('teno', 0.03951367781155015),\n",
       " ('tenn', 0.00911854103343465),\n",
       " ('tank', 0.0060790273556231)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words[5].spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [\n",
    "('Tom Holland is a terrible spiderman.','pos'),\n",
    "('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
    "('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
    "('Fantastic Four should have never been made.','pos'),\n",
    "('Wes Anderson is my favorite director!','neg'),\n",
    "('Captain America 2 is pretty awesome.','neg'),\n",
    "('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
    "]\n",
    "\n",
    "testing = [\n",
    "('Superman was never an interesting character.','pos'),\n",
    "('Fantastic Mr Fox is an awesome film!','neg'),\n",
    "('Dragonball Evolution is simply terrible!!','pos')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import classifiers\n",
    "nb_classifier = classifiers.NaiveBayesClassifier(training)\n",
    "dt_classifier = classifiers.DecisionTreeClassifier(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print (nb_classifier.accuracy(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains(is) = True              neg : pos    =      2.9 : 1.0\n",
      "      contains(terrible) = False             neg : pos    =      1.8 : 1.0\n",
      "             contains(a) = False             neg : pos    =      1.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_informative_features(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob('the weather is terrible!', classifier=dt_classifier)\n",
    "print (blob.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Polarizing Functions for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the polarized lexycom file - OpLexicon (BR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = pd.read_csv(os.path.join(datapath,oplexicon), header=None, encoding='utf-8', usecols=[0,2], names=[u'palavra',u'polaridade'])\n",
    "df_o.drop_duplicates(subset=[u'palavra'], keep='first', inplace=True)\n",
    "df_o = df_o.set_index(u'palavra')\n",
    "print(u'Corpus com {} palavras polarizadas'.format(len(df_o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polariza_texto(texto, df1):\n",
    "    polaridades = []\n",
    "    ausentes = []\n",
    "    l_palavras = texto.split()\n",
    "    for palavra in l_palavras:\n",
    "        p = palavra.lower().strip()\n",
    "        if p in df1.index:\n",
    "            polaridades.append(df1.polaridade[p])\n",
    "        else:\n",
    "            ausentes.append(p)\n",
    "    num_pal = len(l_palavras)\n",
    "    num_pol = len(polaridades)\n",
    "    razao_pol = num_pol/float(num_pal) if num_pal else 0.0\n",
    "    polaridade = sum(polaridades)/float(num_pol) if num_pol else 0.0\n",
    "    #print('Foram polarizadas {} palavras de um total de {} ({:.2%})'.format(num_pol, num_pal, razao_pol))\n",
    "    #print('A polaridade mensurada do texto é de {:.3}'.format(polaridade))\n",
    "    return polaridade, ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polariza_counter(dicionario, df1):\n",
    "    polaridades = []\n",
    "    fator_div = 0\n",
    "    ausentes = []\n",
    "    for palavra, frequencia in dicionario.iteritems():\n",
    "        p = palavra.lower().strip()\n",
    "        if p in df1.index:\n",
    "            polaridades.append(df1.polaridade[p] * frequencia)\n",
    "            fator_div += frequencia\n",
    "        else:\n",
    "            ausentes.append((palavra,frequencia))\n",
    "    num_pal = len(dicionario)\n",
    "    num_pol = len(polaridades)\n",
    "    razao_pol = num_pol/float(num_pal) if num_pal else 0.0\n",
    "    polaridade = sum(polaridades)/float(fator_div)\n",
    "    print('Foram polarizadas {} palavras de um total de {} ({:.2%})'.format(num_pol, num_pal, razao_pol))\n",
    "    print('A polaridade mensurada do texto é de {:.3}'.format(polaridade))\n",
    "    return polaridade, ausentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinando os arquivos e pastas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpuspath = os.path.join(outputs,'html_out')\n",
    "onlydirs = [f for f in os.listdir(corpuspath) if os.path.isdir(os.path.join(corpuspath,f)) and not f.startswith('.')]\n",
    "onlyfiles = [f for f in os.listdir(corpuspath) if os.path.isfile(os.path.join(corpuspath,f)) and not f.startswith('.')]\n",
    "onlydirs.sort()\n",
    "\n",
    "print 'Files in the folder:'\n",
    "for i, w in enumerate(onlyfiles[0:]):\n",
    "    print i+1, '--' ,w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando os corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = ''\n",
    "list_raw_texts = []\n",
    "for filename in onlyfiles:\n",
    "    page_txt = codecs.open(os.path.join(corpuspath,filename), encoding='utf-8').read()\n",
    "    raw_texts += u'\\n'+ page_txt.lower()\n",
    "    list_raw_texts.append(page_txt)\n",
    "len(list_raw_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ocorrências de determinada expressão no corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#expressao = u'nova_classe_média'.lower()\n",
    "#expressao = u'Marcelo Néri'.lower()\n",
    "expressao = u'Brasil'.lower()\n",
    "#expressao = u'índice'.lower()\n",
    "#expressao = u'Instituto de Pesquisa'.lower() #muitas grafias diferentes para o Ipea\n",
    "#expressao = u'Ipea'.lower()\n",
    "#expressao = u'porta dos fundos'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0 \n",
    "positions = []\n",
    "while pos != -1:\n",
    "    position = raw_texts.find(expressao,pos+1)\n",
    "    pos = position\n",
    "    positions.append(position)\n",
    "positions.pop()\n",
    "print(u'A expressão buscada ocorre {} vezes'.format(len(positions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for position in positions: #[0:10]:\n",
    "    print raw_texts[position - 200:position + 200].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um passo opcional - retirar as stopwords que podem interferir nas análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_words = [w for w in nltk.corpus.stopwords.words('portuguese')]\n",
    "#ignore_words.extend([s.decode('utf-8') for s in string.punctuation])\n",
    "ignore_words.extend([u' ', u'', u'é', u'r',u'c',u'ainda',u'vai',u'ser',u'globo',u'sobre',u'nesta',u'\\u2013',u'\\u2014', u'pode',\n",
    "                     u'ter', u'disse'])\n",
    "ignore_expressions = [u'Copyright © 2013', u'Copyright © 2014',u'Todos os direitos reservados', \n",
    "                      u'Agência Estado', u'Jornal O Globo', u'Folha de S.Paulo', 'Globo Digital', u'SEGUNDO CADERNO',\n",
    "                      u'Noblat', u'Agamenon Mendes Pedreira', u'Agamenon', u'Merval Pereira', u'Merval', u'Amaury de Souza',\n",
    "                      u'Boa Viagem', u'v\\xeddeos pol\\xedticos', u'Esta\\xe7\\xe3o Jazz e Tal', u'a r\\xe1dio do blog',\n",
    "                      u'Siga o', u'Leia a', u'Ou\\xe7a a', u'Curta a P\\xe1gina', u'Visite a p\\xe1gina', u'no Twitter','no Facebook', u'Blog do', \n",
    "                      u'Tradu\\xe7\\xe3o', u'mat&eacute;ria na &iacute;ntegra', u'\\xedntegra da mat\\xe9ria', u'para assinantes',\n",
    "                      u'por exemplo', u'cada vez', 'datafolha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in ignore_expressions:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrige_distorcoes(texto):\n",
    "    texto = texto.replace(u'\\xe0', u'a a') #separa o a com crase\n",
    "    texto = texto.replace(u'\\u201c', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'\\u201d', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'\\u2018', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'\\u2019', '') #retira um certo tipo de aspas\n",
    "    texto = texto.replace(u'get\\xfalio vargas', u'getulio vargas') #homogeniza as formas de Getulio Vargas\n",
    "    return texto\n",
    "    \n",
    "def limpa_stopwords(texto):\n",
    "    for expression in ignore_expressions:\n",
    "        texto = texto.replace(expression.lower(),'') #retira as expressoes\n",
    "    lista = [w.strip(string.punctuation) for w in texto.split() if w.strip(string.punctuation) not in ignore_words] #retira stopw.\n",
    "    texto = u' '.join(lista)\n",
    "    return texto, lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = corrige_distorcoes(raw_texts)\n",
    "cleaned_texts, list_cleaned_words = limpa_stopwords(raw_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira coisa a se analisar são - pura e simplesmente - as palavras (sem stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words = Counter(list_cleaned_words)\n",
    "df_freq_words = pd.DataFrame(freq_words.values(), columns = [u'Frequência'], index=freq_words.keys())\n",
    "df_freq_words = df_freq_words.sort_index(by=u'Frequência', ascending=False)\n",
    "df_freq_words.index.name = u'Tokens'\n",
    "df_freq_words[0:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando a polaridade total dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polariza_counter(freq_words, df_o);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos visualizar as palavras mais frequentes de várias formas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vk(lst):\n",
    "    \"\"\"Print a list of value/key pairs nicely formatted in key/value order.\"\"\"\n",
    "\n",
    "    # Find the longest key: remember, the list has value/key pairs, so the key\n",
    "    # is element [1], not [0]\n",
    "    longest_key = max([len(word) for word, count in lst])\n",
    "    # Make a format string out of it\n",
    "    fmt = '%'+str(longest_key)+'s -> %s'\n",
    "    # Do actual printing\n",
    "    for k,v in lst:\n",
    "        print fmt % (k,v)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_histogram(freqs, show=10, title=None):\n",
    "    \"\"\"Plot a histogram of word frequencies, limited to the top <show> ones.\n",
    "    \"\"\"\n",
    "    sorted_f = sort_freqs(freqs) if isinstance(freqs, dict) else freqs\n",
    "\n",
    "    # Don't show the tail\n",
    "    if isinstance(show, int):\n",
    "        # interpret as number of words to show in histogram\n",
    "        show_f = sorted_f[-show:]\n",
    "    else:\n",
    "        # interpret as a fraction\n",
    "        start = -int(round(show*len(freqs)))\n",
    "        show_f = sorted_f[start:]\n",
    "\n",
    "    # Now, extract words and counts, plot\n",
    "    n_words = len(show_f)\n",
    "    ind = np.arange(n_words)\n",
    "    words = [i[0] for i in show_f]\n",
    "    counts = [i[1] for i in show_f]\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    if n_words<=20:\n",
    "        # Only show bars and x labels for small histograms, they don't make\n",
    "        # sense otherwise\n",
    "        ax.bar(ind, counts)\n",
    "        ax.set_xticks(ind)\n",
    "        ax.set_xticklabels(words, rotation=45)\n",
    "        fig.subplots_adjust(bottom=0.25)\n",
    "    else:\n",
    "        # For larger ones, do a step plot\n",
    "        ax.step(ind, counts)\n",
    "\n",
    "    # If it spans more than two decades, use a log scale\n",
    "    if float(max(counts))/min(counts) > 100:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_words=15      \n",
    "w_items = freq_words.items()\n",
    "w_items.sort(key = lambda wc: wc[1])\n",
    "print 'Number of unique words:',len(w_items)\n",
    "#print('{} least frequent words:').format(number_words)\n",
    "#print_vk(w_items[:10])\n",
    "#print('{} most frequent words:').format(number_words)\n",
    "print_vk(w_items[:-10:-1])\n",
    "plot_word_histogram(w_items, number_words,'Frequencies for {} most frequent words'.format(number_words));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_words = nltk.FreqDist(freq_words)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8,8)) \n",
    "fd_words.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E a distribuição cumulativa das frequências:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8,8)) \n",
    "fd_words.plot(30, cumulative = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.array(freq_words.values())\n",
    "words = np.array(freq_words.keys())\n",
    "count = count.astype(int)\n",
    "make_wordcloud(words, count, 'test.png')\n",
    "Image(filename='test.png', width=640, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora construir um grafo de palavras para estudar co-ocorrências nos textos dos feeds\n",
    "\n",
    "Abordagem e funções aproveitadas de https://github.com/ipython/talks/blob/master/notebook/text_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_pairs(items):\n",
    "    \"\"\"Make all unique pairs (order doesn't matter)\"\"\"\n",
    "    pairs = []\n",
    "    nitems = len(items)\n",
    "    for i, wi in enumerate(items):\n",
    "        for j in range(i+1, nitems):\n",
    "            pairs.append((wi, items[j]))\n",
    "    return pairs\n",
    "\n",
    "def co_occurrences(lines, words):\n",
    "    \"\"\"Return histogram of co-occurrences of words in a list of lines.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lines : list\n",
    "      A list of strings considered as 'sentences' to search for co-occurrences.\n",
    "\n",
    "    words : list\n",
    "      A list of words from which all unordered pairs will be constructed and\n",
    "      searched for co-occurrences.\n",
    "    \"\"\"\n",
    "    wpairs = all_pairs(words)\n",
    "\n",
    "    # Now build histogram of co-occurrences\n",
    "    co_occur = {}\n",
    "    for w1, w2 in wpairs:\n",
    "        rx = re.compile('%s .*%s|%s .*%s' % (w1, w2, w2, w1))\n",
    "        co_occur[w1, w2] = sum([1 for line in lines if rx.search(line)])\n",
    "\n",
    "    return co_occur\n",
    "\n",
    "def co_occurrences_graph(word_hist, co_occur, cutoff=0):\n",
    "    \"\"\"Convert a word histogram with co-occurrences to a weighted graph.\n",
    "    Edges are only added if the count is above cutoff.\n",
    "    \"\"\"\n",
    "    g = nx.Graph()\n",
    "    for word, count in word_hist:\n",
    "        g.add_node(word, count=count)\n",
    "    for (w1, w2), count in co_occur.iteritems():\n",
    "        if count<=cutoff:\n",
    "            continue\n",
    "        g.add_edge(w1, w2, weight=count)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 15\n",
    "popular = w_items[-n_nodes:]\n",
    "pop_words = [wc[0] for wc in popular]\n",
    "co_occur = co_occurrences(list_raw_texts, pop_words)\n",
    "wgraph = co_occurrences_graph(popular, co_occur, cutoff=1)\n",
    "wsubgraph = list(nx.connected_component_subgraphs(wgraph))[1] #we have to choose the biggest con. comp.\n",
    "centrality = nx.eigenvector_centrality_numpy(wsubgraph)\n",
    "c = centrality.items()\n",
    "c.sort(key=lambda x:x[1], reverse=True)\n",
    "print '\\nGraph centrality'\n",
    "for node, cent in c:\n",
    "    print \"%15s: %.3g\" % (node, float(cent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rad0 = 0.2\n",
    "def rescale_arr(arr, amin, amax):\n",
    "    \"\"\"Rescale an array to a new range.\n",
    "    Return a new array whose range of values is (amin, amax).\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : array-like\n",
    "    amin : float\n",
    "      new minimum value\n",
    "    amax : float\n",
    "      new maximum value\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> a = np.arange(5)\n",
    "    >>> rescale_arr(a,3,6)\n",
    "    array([ 3.  ,  3.75,  4.5 ,  5.25,  6.  ])\n",
    "    \"\"\"\n",
    "    # old bounds\n",
    "    m = arr.min()\n",
    "    M = arr.max()\n",
    "    # scale/offset\n",
    "    s = float(amax-amin)/(M-m)\n",
    "    d = amin - s*m\n",
    "\n",
    "    # Apply clip before returning to cut off possible overflows outside the\n",
    "    # intended range due to roundoff error, so that we can absolutely guarantee\n",
    "    # that on output, there are no values > amax or < amin.\n",
    "    return np.clip(s*arr+d,amin,amax)\n",
    "\n",
    "def plot_graph(wgraph, pos=None, fig=None, title=None):\n",
    "    \"\"\"Conveniently summarize graph visually\"\"\"\n",
    "\n",
    "    # config parameters\n",
    "    edge_min_width= 3\n",
    "    edge_max_width= 12\n",
    "    label_font = 16\n",
    "    node_font = 18\n",
    "    node_alpha = 0.4\n",
    "    edge_alpha = 0.55\n",
    "    edge_cmap = plt.cm.Spectral\n",
    "\n",
    "    # Create figure\n",
    "    if fig is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "    fig.subplots_adjust(0,0,1)\n",
    "\n",
    "    # Plot nodes with size according to count\n",
    "    sizes = []\n",
    "    degrees = []\n",
    "    for n, d in wgraph.nodes_iter(data=True):\n",
    "        sizes.append(d['count'])\n",
    "        degrees.append(wgraph.degree(n))\n",
    "\n",
    "    sizes = rescale_arr(np.array(sizes, dtype=float), 100, 1000)\n",
    "\n",
    "    # Compute layout and label edges according to weight\n",
    "    pos = nx.spring_layout(wgraph) if pos is None else pos\n",
    "    labels = {}\n",
    "    width = []\n",
    "    for n1, n2, d in wgraph.edges_iter(data=True):\n",
    "        w = d['weight']\n",
    "        labels[n1, n2] = w\n",
    "        width.append(w)\n",
    "\n",
    "    width = rescale_arr(np.array(width, dtype=float), edge_min_width, \n",
    "                        edge_max_width)\n",
    "\n",
    "    # Draw\n",
    "    nx.draw_networkx_nodes(wgraph, pos, node_size=sizes, node_color=degrees,alpha=node_alpha)\n",
    "    nx.draw_networkx_edges(wgraph, pos, width=width, edge_color=width, edge_cmap=edge_cmap, alpha=edge_alpha)\n",
    "    nx.draw_networkx_edge_labels(wgraph, pos, edge_labels=labels,font_size=label_font)\n",
    "    nx.draw_networkx_labels(wgraph, pos, font_size=node_font, font_weight='bold')\n",
    "    if title is not None:\n",
    "        ax.set_title(title, fontsize=label_font)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Mark centrality axes\n",
    "    kw = dict(color='k', linestyle='-')\n",
    "    cross = [ax.axhline(0, **kw), ax.axvline(rad0, **kw)]\n",
    "    [ l.set_zorder(0) for l in cross]\n",
    "    \n",
    "def centrality_layout(wgraph, centrality):\n",
    "    \"\"\"Compute a layout based on centrality.\n",
    "    \"\"\"\n",
    "    # Create a list of centralities, sorted by centrality value\n",
    "    cent = sorted(centrality.items(), key=lambda x:float(x[1]), reverse=True)\n",
    "    nodes = [c[0] for c in cent]\n",
    "    cent  = np.array([float(c[1]) for c in cent])\n",
    "    rad = (cent - cent[0])/(cent[-1]-cent[0])\n",
    "    rad = rescale_arr(rad, rad0, 1)\n",
    "    angles = np.linspace(0, 2*np.pi, len(centrality))\n",
    "    layout = {}\n",
    "    for n, node in enumerate(nodes):\n",
    "        r = rad[n]\n",
    "        th = angles[n]\n",
    "        layout[node] = r*np.cos(th), r*np.sin(th)\n",
    "    return layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Graph visualization for query:\"\n",
    "plot_graph(wsubgraph, centrality_layout(wsubgraph, centrality), \n",
    "           plt.figure(figsize=(12,12)), \n",
    "           title = u'Centrality and term co-occurrence graph, q=\"{}\"'.format(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender os contextos, uma análise de concordâncias\n",
    "(palavras associadas à expressão escolhida, posicionadas na mesma sentença):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = nltk.tokenize.WordPunctTokenizer()\n",
    "tokens = tknzr.tokenize(raw_texts) # texto com stopwords\n",
    "#tokens = tknzr.tokenize(cleaned_texts) # texto sem stopwords\n",
    "nltk_text = nltk.Text(tokens)\n",
    "nltk_text.concordance(query.lower(), width=120, lines=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Palavras que ocorrem em contextos similares (associadas às mesmas palavras que a consulta):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.similar(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As colocações (que não são co-ocorrências) nos informam sobre as palavras (quaisquer, não necessariamente ligadas à consulta) que ocorrem conjuntamente (bigramas frequentes):\n",
    "http://en.wikipedia.org/wiki/Collocation\n",
    "http://en.wikipedia.org/wiki/Co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocations(num=100, window_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations are expressions of multiple words which commonly co-occur.\n",
    "\n",
    "Below we are using Pointwise Mutual Information.\n",
    "\n",
    "http://en.wikipedia.org/wiki/Pointwise_mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_items = 50\n",
    "freq_min_b = 10\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder2 = nltk.collocations.BigramCollocationFinder.from_words(nltk_text)\n",
    "finder2.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder2.apply_freq_filter(freq_min_b)\n",
    "for a, b in finder2.nbest(bigram_measures.pmi, max_items):\n",
    "    print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_items = 50\n",
    "freq_min_t = 6\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder3 = nltk.collocations.TrigramCollocationFinder.from_words(nltk_text)\n",
    "finder3.apply_word_filter(lambda w: w in string.punctuation)\n",
    "finder3.apply_ngram_filter(lambda w1, w2, w3:  w1 in ['da', 'de', 'das'])\n",
    "finder3.apply_freq_filter(freq_min_t)\n",
    "for a, b, c in finder3.nbest(trigram_measures.pmi, max_items):\n",
    "    print a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b in (finder2.above_score(bigram_measures.raw_freq,1.0 / len(list(nltk.bigrams(tokens))))):\n",
    "        print a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, b, c in finder3.above_score(trigram_measures.raw_freq,1.0 / len(list(nltk.trigrams(tokens)))):\n",
    "        print a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "n = 4\n",
    "xgrams = ngrams(tokens, n)\n",
    "xgrams_counter = Counter(xgrams)\n",
    "df_ngrams = pd.DataFrame(xgrams_counter.items(), columns = [u'n-gramas',u'Frequência'])\n",
    "df_ngrams = df_ngrams.sort_index(by=u'Frequência', ascending=False)\n",
    "df_ngrams.set_index([u'n-gramas'], inplace=True)\n",
    "df_ngrams[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.dispersion_plot([u'fgv', \n",
    "                      u'ipea', \n",
    "                      u'ibge',\n",
    "                      u'aécio',\n",
    "                      u'dilma',\n",
    "                      u'lula',\n",
    "                      u'renda',\n",
    "                      u'cps-fgv',\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções a seguir permitem extrair as frases mais significativas do texto (sumarização automática)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(sentences, important_words):\n",
    "    # Approach taken from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
    "    CLUSTER_THRESHOLD = 5  # Distance between words to consider\n",
    "    scores = []\n",
    "    sentence_idx = -1\n",
    "    #for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "    punktw2 = nltk.tokenize.WordPunctTokenizer() #trocando pelo punkt (melhor?)\n",
    "    for s in [punktw2.tokenize(s) for s in sentences]:    \n",
    "        sentence_idx += 1\n",
    "        word_idx = []\n",
    "        # For each word in the word list...\n",
    "        for w in important_words:\n",
    "            try:\n",
    "                # Compute an index for where any important words occur in the sentence\n",
    "                word_idx.append(s.index(w))\n",
    "            except ValueError, e: # w not in this particular sentence\n",
    "                pass\n",
    "        word_idx.sort()\n",
    "        # It is possible that some sentences may not contain any important words at all\n",
    "        if len(word_idx)== 0: continue\n",
    "        # Using the word index, compute clusters by using a max distance threshold\n",
    "        # for any two consecutive words\n",
    "        clusters = []\n",
    "        cluster = [word_idx[0]]\n",
    "        i = 1\n",
    "        while i < len(word_idx):\n",
    "            if word_idx[i] - word_idx[i - 1] < CLUSTER_THRESHOLD:\n",
    "                cluster.append(word_idx[i])\n",
    "            else:\n",
    "                clusters.append(cluster[:])\n",
    "                cluster = [word_idx[i]]\n",
    "            i += 1\n",
    "        clusters.append(cluster)\n",
    "        # Score each cluster. The max score for any given cluster is the score \n",
    "        # for the sentence\n",
    "        max_cluster_score = 0\n",
    "        for c in clusters:\n",
    "            significant_words_in_cluster = len(c)\n",
    "            total_words_in_cluster = c[-1] - c[0] + 1\n",
    "            score = 1.0 * significant_words_in_cluster \\\n",
    "                * significant_words_in_cluster / total_words_in_cluster\n",
    "            if score > max_cluster_score:\n",
    "                max_cluster_score = score\n",
    "        scores.append((sentence_idx, score))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(txt):\n",
    "    TOP_SENTENCES = 10  # Number of sentences to choose on \"top n\"\n",
    "    N = 100  # Number of words to consider\n",
    "    #sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    punkts = nltk.tokenize.PunktSentenceTokenizer() #trocando pelo punkt (melhor?)\n",
    "    punktw = nltk.tokenize.WordPunctTokenizer() #trocando pelo punkt (melhor?)\n",
    "    sentences = [s for s in punkts.tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in punktw.tokenize(sentence)]\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    top_n_words = [w[0] for w in fdist.items() if w[0] not in ignore_words][:N]\n",
    "    scored_sentences = score_sentences(normalized_sentences, top_n_words)\n",
    "    # First approach:\n",
    "    # Filter out non-significant sentences by using the average score plus a\n",
    "    # fraction of the std dev as a filter\n",
    "    avg = np.mean([s[1] for s in scored_sentences])\n",
    "    std = np.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
    "                   if score > avg + 0.5 * std]\n",
    "    # Second Approach: \n",
    "    # Return only the top N ranked sentences\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-TOP_SENTENCES:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "    # Decorate the post object with summaries\n",
    "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumarios = summarize(raw_texts)\n",
    "sumarios['top_n_summary'] = set(sumarios['top_n_summary'])\n",
    "sumarios['mean_scored_summary'] = set(sumarios['mean_scored_summary'])\n",
    "\n",
    "print(u'frases mais importantes(1):\\n')\n",
    "for s in sumarios['top_n_summary']:\n",
    "    print(u'{}\\n'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u'\\nfrases mais importantes(2):\\n')\n",
    "for s in sumarios['mean_scored_summary']:\n",
    "    print(u'{}\\n'.format(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extração de palavras relevantes usando TF-IDf  \n",
    "http://radimrehurek.com/gensim/models/tfidfmodel.html  \n",
    "http://radimrehurek.com/gensim/tutorial.html  \n",
    "http://radimrehurek.com/gensim/tut2.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_once = [key for key in freq_words.keys() if freq_words[key]==1]\n",
    "texts = [[word.strip(string.punctuation) for word in document.lower().split() if word not in ignore_words]\n",
    "         for document in list_raw_texts]\n",
    "texts = [[word for word in text if word not in tokens_once and len(word) > 1] for text in texts]\n",
    "dictionary = gensim.corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "tfidf = gensim.models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "for doc in corpus_tfidf:\n",
    "    print(u'\\nRelevant Words in Document {}:\\n'.format(x))\n",
    "    relev = 0\n",
    "    top_w = ''\n",
    "    x+=1\n",
    "    for w_in_dic, tfidf_w in doc:\n",
    "        if tfidf_w > relev:\n",
    "            top_w = w_in_dic\n",
    "            relev = tfidf_w\n",
    "    if top_w != '' and relev > 0:\n",
    "        print('{}\\t{}'.format(dictionary[top_w], relev)) #Rever questão do UTF-8\n",
    "    else:\n",
    "        print('Nothing relevant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
